{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "xWYY7FYfQyAD",
    "fKEeTGgOaaV3",
    "8VS-MOgda5TE",
    "ZpY3ar1kk32r"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantized Bonito Tutorial\n",
    "This tutorial shows how to run a quantized version of [Bonito](https://github.com/BatsResearch/bonito) on a Google Colab T4 instance using the `transformers` package (instead of `vllm` as in the original repo). We use the quantized model from [alexandreteles/bonito-v1-awq](https://huggingface.co/alexandreteles/bonito-v1-awq). Note that the quantized models may behave differently than their non-quantized counterparts.\n",
    "\n",
    "If you wish to run the original Bonito model on A100 GPUs, check out [this tutorial](https://colab.research.google.com/drive/1XuDRVKpUUqdjrqg2-P2FIqkdAQBnqoNL?usp=sharing).\n"
   ],
   "metadata": {
    "id": "-K1cD9V8SDIG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "First we clone into the repo and install the dependencies. This will take several minutes."
   ],
   "metadata": {
    "id": "Gyh5HAFxQlaH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lqD8IrM8Vo0"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/BatsResearch/bonito.git\n",
    "!pip install -U bonito/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use this quantized model, we need to install the [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) package, which deals with AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)) models, such as the one we'll be using. AWQ is a quantization technique that treats different weight parameters differently based on their importance. To get it to work with Colab, we have to install the kernel from a specialized wheel so the CUDA versions match."
   ],
   "metadata": {
    "id": "13b49FjqvRur"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install autoawq\n",
    "!git clone https://github.com/Boltuzamaki/AutoAWQ_kernels.git\n",
    "!pip install AutoAWQ_kernels/builds/autoawq_kernels-0.0.6+cu122-cp310-cp310-linux_x86_64.whl"
   ],
   "metadata": {
    "id": "5Y6L6xYP1KTe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantized Bonito Wrapper\n",
    "This is a simplified quantized bonito class to generate a single synthetic input-output instruction for a given text and task type.\n",
    "This code uses huggingface `transformers` library for generation.\n",
    "For complete functionality and faster generations, we recommend using the `Bonito` class from the package."
   ],
   "metadata": {
    "id": "xWYY7FYfQyAD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Optional, List, Dict\n",
    "from datasets import Dataset\n",
    "from awq import AutoAWQForCausalLM\n",
    "from bonito import AbstractBonito\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class QuantizedBonito(AbstractBonito):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        self.model = AutoAWQForCausalLM.from_quantized(\n",
    "            model_name_or_path, fuse_layers=True\n",
    "        ).cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def generate_task(\n",
    "        self,\n",
    "        unannotated_paragraph: str,\n",
    "        task_type: str,\n",
    "        sampling_params: dict,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generates synthetic instruction tuning pair using the Quantized Bonito model.\n",
    "        This method takes a text unannotated text, a task type, and sampling parameters,\n",
    "        and generates synthetic input-output pair.\n",
    "\n",
    "        Args:\n",
    "            unannotated_paragraph (str): The unannotated text or a paragraph\n",
    "            task_type (str): The type of the tasks. This can be a\n",
    "                short form or a full form.\n",
    "            sampling_params (dict): The parameters for\n",
    "                sampling.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The synthetic input-output pair for the task type.\n",
    "        \"\"\"\n",
    "\n",
    "        text_dataset = Dataset.from_list([{\"input\": unannotated_paragraph}])\n",
    "\n",
    "        processed_dataset = self._prepare_bonito_input(\n",
    "            text_dataset, task_type, context_col=\"input\"\n",
    "        )\n",
    "\n",
    "        outputs = self._generate_text(processed_dataset[\"input\"], sampling_params)\n",
    "        examples = []\n",
    "        for i, example in enumerate(text_dataset.to_list()):\n",
    "            output = outputs[i]\n",
    "            example[\"prediction\"] = output.strip()\n",
    "            examples.append(example)\n",
    "\n",
    "        synthetic_dataset = Dataset.from_list(examples)\n",
    "\n",
    "        # filter out the examples that cannot be parsed\n",
    "        synthetic_dataset_dict = self._postprocess_dataset(\n",
    "            synthetic_dataset, context_col=\"input\"\n",
    "        ).to_list()[0]\n",
    "\n",
    "        return synthetic_dataset_dict\n",
    "\n",
    "    def _generate_text(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        sampling_params: dict,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text using huggingface transformers generate function.\n",
    "\n",
    "        This method takes a dataset of prompts, encodes them,\n",
    "        generates text using the model, decodes the generated\n",
    "        text, and appends it to a list.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): A dataset containing prompts for text generation.\n",
    "            sampling_params (dict): Parameters for sampling during generation.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of generated texts corresponding to the prompts.\n",
    "        \"\"\"\n",
    "        generated_texts = []\n",
    "\n",
    "        for prompt in dataset:\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            input_ids = input_ids.cuda()\n",
    "\n",
    "            output = self.model.generate(input_ids, do_sample=True, **sampling_params)\n",
    "\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                output[0][len(input_ids[0]) :], skip_special_tokens=True\n",
    "            )\n",
    "            generated_texts.append(generated_text)\n",
    "\n",
    "        return generated_texts"
   ],
   "metadata": {
    "id": "NmsYTEdR-m59"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Bonito Model\n",
    "Load the quantized Bonito model from HuggingFace Hub."
   ],
   "metadata": {
    "id": "fKEeTGgOaaV3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bonito = QuantizedBonito(\"alexandreteles/bonito-v1-awq\")"
   ],
   "metadata": {
    "id": "_s65C8P5adUz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthetic Data Generation\n",
    "Here we will load the quantized bonito model and generate synthetic instruction for the unannotated text.\n",
    "\n"
   ],
   "metadata": {
    "id": "86OvwN74RcS8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Text\n",
    "We select the sample text from the ContractNLI dataset. You can replace this text as you wish."
   ],
   "metadata": {
    "id": "8VS-MOgda5TE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "unannotated_paragraph = \"\"\"1. “Confidential Information”, whenever used in this Agreement, shall mean any data, document, specification and other information \\nor material, that is delivered or disclosed by UNHCR to the Recipient in any form whatsoever, whether orally, visually in writing \\nor otherwise (including computerized form), and that, at the time of disclosure to the Recipient, is designated as \\nconfidential.\"\"\"\n",
    "print(unannotated_paragraph)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfOlo_Df3Wl_",
    "outputId": "90a5b7bc-9947-4165-ba78-6828089ab480"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1. “Confidential Information”, whenever used in this Agreement, shall mean any data, document, specification and other information \n",
      "or material, that is delivered or disclosed by UNHCR to the Recipient in any form whatsoever, whether orally, visually in writing \n",
      "or otherwise (including computerized form), and that, at the time of disclosure to the Recipient, is designated as \n",
      "confidential.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the synthetic instructions\n",
    "After loading the model, we pass the unannotated paragraph and the task type to generate the instructions.\n",
    "Here we generate an NLI task:"
   ],
   "metadata": {
    "id": "ZpY3ar1kk32r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# making predictions deterministic.\n",
    "set_seed(2)\n",
    "\n",
    "# Generate synthetic instruction tuning dataset\n",
    "sampling_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "synthetic_dataset = bonito.generate_task(\n",
    "    unannotated_paragraph, task_type=\"nli\", sampling_params=sampling_params\n",
    ")\n",
    "print(\"----Generated Instructions----\")\n",
    "print(f'Input: {synthetic_dataset[\"input\"]}')\n",
    "print(f'Output: {synthetic_dataset[\"output\"]}')"
   ],
   "metadata": {
    "id": "k4lreUPb0LUX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we change the task type from NLI (nli) to multiple choice question answering (mcqa). For more details on task types, see [supported task types](https://github.com/BatsResearch/bonito?tab=readme-ov-file#supported-task-types)"
   ],
   "metadata": {
    "id": "_IKrfr3AmSGn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# making predictions deterministic.\n",
    "set_seed(55)\n",
    "sampling_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "synthetic_dataset = bonito.generate_task(\n",
    "    unannotated_paragraph, task_type=\"mcqa\", sampling_params=sampling_params  # changed\n",
    ")\n",
    "print(\"----Generated Instructions----\")\n",
    "print(f'Input: {synthetic_dataset[\"input\"]}')\n",
    "print(f'Output: {synthetic_dataset[\"output\"]}')"
   ],
   "metadata": {
    "id": "2l2OR9i_mLlw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now go try it out with your own datasets! You can vary the `task_type` for different types of generated instructions.\n",
    "You can also play around the sampling hyperparameters such as `top_p` and `temperature`."
   ],
   "metadata": {
    "id": "mEU1lp5TVjGj"
   }
  }
 ]
}