{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1cD9V8SDIG"
      },
      "source": [
        "# Quantized Bonito Tutorial\n",
        "This is a tutorial to set up and run a quantized version of [Bonito](https://github.com/BatsResearch/bonito) on a Google Colab T4 instance using the `transformers` package (instead of `vllm` as in the original repo). The quantized model was graciously created by GitHub/HuggingFace user `alexandreteles` and we thank them for their contributions! Note that quantized models may behave differently than their non-quantized counterparts. The versions they created are:\n",
        " - [alexandreteles/bonito-v1-awq](https://huggingface.co/alexandreteles/bonito-v1-awq) (`awq` quantized model, this is the one we'll be using)\n",
        " - [alexandreteles/bonito-v1-gguf](https://huggingface.co/alexandreteles/bonito-v1-gguf) (for llama.cpp inference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyh5HAFxQlaH"
      },
      "source": [
        "## Setup\n",
        "First we clone into the repo and install the dependencies. This will take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lqD8IrM8Vo0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/BatsResearch/bonito.git\n",
        "!pip install -e bonito/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b49FjqvRur"
      },
      "source": [
        "To use this quantized model, we need to install the [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) package, which deals with AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)) models, such as the one we'll be using. AWQ is a quantization technique that treats different weight parameters differently based on their importance. To get it to work with Colab, we have to install the kernel from a specialized wheel so the CUDA versions match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6L6xYP1KTe"
      },
      "outputs": [],
      "source": [
        "!pip install autoawq\n",
        "!git clone https://github.com/Boltuzamaki/AutoAWQ_kernels.git\n",
        "!pip install AutoAWQ_kernels/builds/autoawq_kernels-0.0.6+cu122-cp310-cp310-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWYY7FYfQyAD"
      },
      "source": [
        "## Quantized Bonito Wrapper\n",
        "This cell includes the code to work with the quantized Bonito model, utilizing the `transformers` package. It's similar to the `Bonito` code made for `vllm` in the repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NmsYTEdR-m59"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "from datasets import Dataset\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "SHORTFORM_TO_FULL_TASK_TYPES = {\n",
        "    \"exqa\": \"extractive question answering\",\n",
        "    \"mcqa\": \"multiple-choice question answering\",\n",
        "    \"qg\": \"question generation\",\n",
        "    \"qa\": \"question answering without choices\",\n",
        "    \"ynqa\": \"yes-no question answering\",\n",
        "    \"coref\": \"coreference resolution\",\n",
        "    \"paraphrase\": \"paraphrase generation\",\n",
        "    \"paraphrase_id\": \"paraphrase identification\",\n",
        "    \"sent_comp\": \"sentence completion\",\n",
        "    \"sentiment\": \"sentiment\",\n",
        "    \"summarization\": \"summarization\",\n",
        "    \"text_gen\": \"text generation\",\n",
        "    \"topic_class\": \"topic classification\",\n",
        "    \"wsd\": \"word sense disambiguation\",\n",
        "    \"te\": \"textual entailment\",\n",
        "    \"nli\": \"natural language inference\",\n",
        "}\n",
        "\n",
        "\n",
        "class QuantizedBonito():\n",
        "    def __init__(self, model_name_or_path):\n",
        "        self.model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True).cuda()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "    def generate_tasks(\n",
        "        self,\n",
        "        text_dataset: Dataset,\n",
        "        context_col: str,\n",
        "        task_type: str,\n",
        "        sampling_params: dict,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generates tasks using the Bonito model.\n",
        "\n",
        "        This method takes a text dataset, a context column name,\n",
        "        a task type, and sampling parameters, and generates tasks\n",
        "        using the Bonito model. It processes the input dataset,\n",
        "        generates outputs, collects multiple generations into\n",
        "        one dataset object, and filters out the examples that\n",
        "        cannot be parsed.\n",
        "\n",
        "        Args:\n",
        "            text_dataset (Dataset): The dataset that provides the text\n",
        "                for the tasks.\n",
        "            context_col (str): The name of the column in the dataset\n",
        "                that provides the context for the tasks.\n",
        "            task_type (str): The type of the tasks. This can be a\n",
        "                short form or a full form.\n",
        "            sampling_params (dict): The parameters for\n",
        "                sampling.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The synthetic dataset with the generated tasks.\n",
        "        \"\"\"\n",
        "        processed_dataset = self._prepare_bonito_input(\n",
        "            text_dataset, task_type, context_col, **kwargs\n",
        "        )\n",
        "\n",
        "        outputs = self._generate_text(processed_dataset[\"input\"], sampling_params)\n",
        "\n",
        "        # collect multiple generations into one dataset object\n",
        "        examples = []\n",
        "        for i, example in enumerate(text_dataset.to_list()):\n",
        "            output = outputs[i]\n",
        "            example[\"prediction\"] = output.strip()\n",
        "            examples.append(example)\n",
        "\n",
        "        synthetic_dataset = Dataset.from_list(examples)\n",
        "\n",
        "        # filter out the examples that cannot be parsed\n",
        "        synthetic_dataset = self._postprocess_dataset(\n",
        "            synthetic_dataset, context_col, **kwargs\n",
        "        )\n",
        "\n",
        "        return synthetic_dataset\n",
        "\n",
        "    def _generate_text(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        sampling_params: dict,\n",
        "        ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate text using the model.\n",
        "\n",
        "        This method takes a dataset of prompts, encodes them,\n",
        "        generates text using the model, decodes the generated\n",
        "        text, and appends it to a list.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): A dataset containing prompts for text generation.\n",
        "            sampling_params (dict): Parameters for sampling during generation.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: A list of generated texts corresponding to the prompts.\n",
        "        \"\"\"\n",
        "        generated_texts = []\n",
        "\n",
        "        for prompt in dataset:\n",
        "            input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "            input_ids = input_ids.cuda()\n",
        "\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                do_sample=True,\n",
        "                **sampling_params\n",
        "            )\n",
        "\n",
        "            generated_text = self.tokenizer.decode(output[0][len(input_ids[0]):], skip_special_tokens=True)\n",
        "            generated_texts.append(generated_text)\n",
        "\n",
        "        return generated_texts\n",
        "\n",
        "\n",
        "    def _prepare_bonito_input(\n",
        "        self, context_dataset: Dataset, task_type: str, context_col: str, **kwargs\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        Prepares the input for the Bonito model.\n",
        "\n",
        "        This method takes a context dataset, a task type, and a context\n",
        "        column name, and prepares the dataset for the Bonito model.\n",
        "        If the task type is not recognized, it raises a ValueError.\n",
        "\n",
        "        Args:\n",
        "            context_dataset (Dataset): The dataset that provides the\n",
        "                context for the task.\n",
        "            task_type (str): The type of the task. This can be a\n",
        "                short form or a full form. If the task type is not\n",
        "                recognized, a ValueError is raised.\n",
        "            context_col (str): The name of the column in the dataset\n",
        "                that provides the context for the task.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The prepared dataset for the Bonito model.\n",
        "        \"\"\"\n",
        "        # get the task type name\n",
        "        if task_type in SHORTFORM_TO_FULL_TASK_TYPES.values():\n",
        "            full_task_type = task_type\n",
        "        elif task_type in SHORTFORM_TO_FULL_TASK_TYPES:\n",
        "            full_task_type = SHORTFORM_TO_FULL_TASK_TYPES[task_type]\n",
        "        else:\n",
        "            raise ValueError(f\"Task type {task_type} not recognized\")\n",
        "\n",
        "        def process(example):\n",
        "            input_text = \"<|tasktype|>\\n\" + full_task_type.strip()\n",
        "            input_text += (\n",
        "                \"\\n<|context|>\\n\" + example[context_col].strip() + \"\\n<|task|>\\n\"\n",
        "            )\n",
        "            return {\n",
        "                \"input\": input_text,\n",
        "            }\n",
        "\n",
        "        return context_dataset.map(\n",
        "            process,\n",
        "            remove_columns=context_dataset.column_names,\n",
        "            num_proc=kwargs.get(\"num_proc\", 1),\n",
        "        )\n",
        "\n",
        "    def _postprocess_dataset(\n",
        "        self, synthetic_dataset: Dataset, context_col: str, **kwargs\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        Post-processes the synthetic dataset.\n",
        "\n",
        "        This method takes a synthetic dataset and a context column\n",
        "        name, and post-processes the dataset. It filters out\n",
        "        examples where the prediction does not contain exactly two\n",
        "        parts separated by \"<|pipe|>\", and then maps each example to a\n",
        "        new format where the context is inserted into the first part of\n",
        "        the prediction and the second part of the prediction is used as\n",
        "        the output.\n",
        "\n",
        "        Args:\n",
        "            synthetic_dataset (Dataset): The synthetic dataset to be\n",
        "                post-processed.\n",
        "            context_col (str): The name of the column in the dataset\n",
        "                that provides the context for the tasks.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            Dataset: The post-processed synthetic dataset.\n",
        "        \"\"\"\n",
        "        synthetic_dataset = synthetic_dataset.filter(\n",
        "            lambda example: len(example[\"prediction\"].split(\"<|pipe|>\")) == 2\n",
        "        )\n",
        "\n",
        "        def process(example):\n",
        "            pair = example[\"prediction\"].split(\"<|pipe|>\")\n",
        "            context = example[context_col].strip()\n",
        "            return {\n",
        "                \"input\": pair[0].strip().replace(\"{{context}}\", context),\n",
        "                \"output\": pair[1].strip(),\n",
        "            }\n",
        "\n",
        "        column_names = synthetic_dataset.column_names\n",
        "        processed_synthetic_dataset = synthetic_dataset.map(\n",
        "            process,\n",
        "            remove_columns=column_names,\n",
        "            num_proc=kwargs.get(\"num_proc\", 1),\n",
        "        )\n",
        "\n",
        "        return processed_synthetic_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86OvwN74RcS8"
      },
      "source": [
        "## Synthetic Data Generation\n",
        "This is where we load in the model and unannotated dataset. With them, we can generate a synthetic dataset of instructions. This example generates synthetic instructions from a subset of size 10 of the unannotated dataset. Note that `sampling_params` is modified to use `transformers` keywords instead of `vllm`'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4lreUPb0LUX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Initialize the Bonito model\n",
        "bonito = QuantizedBonito(\"alexandreteles/bonito-v1-awq\")\n",
        "\n",
        "# load dataset with unannotated text\n",
        "unannotated_text = load_dataset(\n",
        "    \"BatsResearch/bonito-experiment\",\n",
        "    \"unannotated_contract_nli\"\n",
        ")[\"train\"].select(range(10))\n",
        "\n",
        "# Generate synthetic instruction tuning dataset\n",
        "sampling_params = {'max_new_tokens':256, 'top_p':0.95, 'temperature':0.5, 'num_return_sequences':1}\n",
        "synthetic_dataset = bonito.generate_tasks(\n",
        "    unannotated_text,\n",
        "    context_col=\"input\",\n",
        "    task_type=\"nli\",\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "\n",
        "print(synthetic_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEU1lp5TVjGj"
      },
      "source": [
        "Now go try it out with your own datasets! You can vary the `task_type` for different types of generated instructions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "zero-shift",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0 | packaged by conda-forge | (default, Nov 26 2020, 07:55:15) \n[Clang 11.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "346a5e91c5f8ad4f8eff3966c4562c80ad00d9220d6c3c49b6573b9ba7f5857f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
